{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27feff80",
   "metadata": {},
   "source": [
    "# Knowledge Graph Link Prediction: Family Relationships\n",
    "\n",
    "\n",
    "This notebook trains both **KG Embedding methods** (TransE, DistMult) and a **GNN-based approach** (R-GCN) to predict missing family relationships and evaluates them using standard metrics (MRR, Hits@1, Hits@10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a37ac",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f2169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056bfe9",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore the Knowledge Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15db4071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training triplets: 13821\n",
      "Test triplets: 590\n",
      "\n",
      "First 5 training triplets:\n",
      "  olivia0 -> sisterOf -> selina10\n",
      "  olivia0 -> sisterOf -> isabella11\n",
      "  olivia0 -> sisterOf -> oskar24\n",
      "  olivia0 -> sisterOf -> adam9\n",
      "  olivia0 -> secondAuntOf -> lena18\n",
      "\n",
      "Total unique entities: 1316\n",
      "Total unique relations: 28\n",
      "\n",
      "Relations: ['auntOf', 'boyCousinOf', 'boyFirstCousinOnceRemovedOf', 'boySecondCousinOf', 'brotherOf', 'daughterOf', 'fatherOf', 'girlCousinOf', 'girlFirstCousinOnceRemovedOf', 'girlSecondCousinOf', 'granddaughterOf', 'grandfatherOf', 'grandmotherOf', 'grandsonOf', 'greatAuntOf', 'greatGranddaughterOf', 'greatGrandfatherOf', 'greatGrandmotherOf', 'greatGrandsonOf', 'greatUncleOf', 'motherOf', 'nephewOf', 'nieceOf', 'secondAuntOf', 'secondUncleOf', 'sisterOf', 'sonOf', 'uncleOf']\n",
      "\n",
      "Sample entities: ['adam1073', 'adam125', 'adam1281', 'adam198', 'adam306', 'adam359', 'adam426', 'adam474', 'adam627', 'adam719']\n"
     ]
    }
   ],
   "source": [
    "def load_triplets(file_path):\n",
    "    \"\"\"Load triplets from a file in format: head relation tail\"\"\"\n",
    "    triplets = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                head, relation, tail = parts\n",
    "                triplets.append((head, relation, tail))\n",
    "    return triplets\n",
    "\n",
    "# Load training and test data\n",
    "train_triplets = load_triplets('train.txt')\n",
    "test_triplets = load_triplets('test.txt')\n",
    "\n",
    "print(f\"Training triplets: {len(train_triplets)}\")\n",
    "print(f\"Test triplets: {len(test_triplets)}\")\n",
    "print(f\"\\nFirst 5 training triplets:\")\n",
    "for i, (h, r, t) in enumerate(train_triplets[:5]):\n",
    "    print(f\"  {h} -> {r} -> {t}\")\n",
    "\n",
    "# Extract entities and relations\n",
    "all_triplets = train_triplets + test_triplets\n",
    "entities = set()\n",
    "relations = set()\n",
    "\n",
    "for head, relation, tail in all_triplets:\n",
    "    entities.add(head)\n",
    "    entities.add(tail)\n",
    "    relations.add(relation)\n",
    "\n",
    "entities = sorted(list(entities))\n",
    "relations = sorted(list(relations))\n",
    "\n",
    "print(f\"\\nTotal unique entities: {len(entities)}\")\n",
    "print(f\"Total unique relations: {len(relations)}\")\n",
    "print(f\"\\nRelations: {relations}\")\n",
    "print(f\"\\nSample entities: {entities[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b8931",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fdb06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity to ID mapping created: 1316 entities\n",
      "Relation to ID mapping created: 28 relations\n",
      "\n",
      "Train triplets shape: (13821, 3)\n",
      "Test triplets shape: (590, 3)\n",
      "\n",
      "Configuration:\n",
      "  Number of entities: 1316\n",
      "  Number of relations: 28\n",
      "  Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "# Create mappings from entities and relations to integer indices\n",
    "entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation2id = {relation: idx for idx, relation in enumerate(relations)}\n",
    "\n",
    "id2entity = {idx: entity for entity, idx in entity2id.items()}\n",
    "id2relation = {idx: relation for relation, idx in relation2id.items()}\n",
    "\n",
    "print(f\"Entity to ID mapping created: {len(entity2id)} entities\")\n",
    "print(f\"Relation to ID mapping created: {len(relation2id)} relations\")\n",
    "\n",
    "# Convert triplets to integer IDs\n",
    "def triplets_to_ids(triplets):\n",
    "    \"\"\"Convert triplet strings to integer IDs\"\"\"\n",
    "    id_triplets = []\n",
    "    for head, relation, tail in triplets:\n",
    "        head_id = entity2id[head]\n",
    "        relation_id = relation2id[relation]\n",
    "        tail_id = entity2id[tail]\n",
    "        id_triplets.append((head_id, relation_id, tail_id))\n",
    "    return np.array(id_triplets)\n",
    "\n",
    "train_triplets_ids = triplets_to_ids(train_triplets)\n",
    "test_triplets_ids = triplets_to_ids(test_triplets)\n",
    "\n",
    "print(f\"\\nTrain triplets shape: {train_triplets_ids.shape}\")\n",
    "print(f\"Test triplets shape: {test_triplets_ids.shape}\")\n",
    "\n",
    "# Store configuration\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "embedding_dim = 100  # Embedding dimension for KG embedding models\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Number of entities: {num_entities}\")\n",
    "print(f\"  Number of relations: {num_relations}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d866e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadfce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_link_prediction(model, test_triplets, scoring_fn, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate link prediction model on test set\n",
    "    Returns: MRR, Hits@1, Hits@10\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mrr_scores = []\n",
    "        hits_1 = 0\n",
    "        hits_10 = 0\n",
    "        total = 0\n",
    "        \n",
    "        for head_id, relation_id, tail_id in test_triplets:\n",
    "            # Score all possible tails for this (head, relation) pair\n",
    "            scores = []\n",
    "            for candidate_id in range(num_entities):\n",
    "                if method_name == \"rgcn\":\n",
    "                    score = scoring_fn(model, head_id, relation_id, candidate_id)\n",
    "                else:\n",
    "                    score = scoring_fn(head_id, relation_id, candidate_id)\n",
    "                scores.append(score.item() if isinstance(score, torch.Tensor) else score)\n",
    "            \n",
    "            scores = np.array(scores)\n",
    "            \n",
    "            # Rank the true tail\n",
    "            sorted_indices = np.argsort(-scores)  # Sort in descending order\n",
    "            rank = np.where(sorted_indices == tail_id)[0][0] + 1  # 1-indexed rank\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mrr_scores.append(1.0 / rank)\n",
    "            if rank == 1:\n",
    "                hits_1 += 1\n",
    "            if rank <= 10:\n",
    "                hits_10 += 1\n",
    "            total += 1\n",
    "        \n",
    "        mrr = np.mean(mrr_scores)\n",
    "        hits_1_metric = hits_1 / total\n",
    "        hits_10_metric = hits_10 / total\n",
    "        \n",
    "        return {\n",
    "            'MRR': mrr,\n",
    "            'Hits@1': hits_1_metric,\n",
    "            'Hits@10': hits_10_metric\n",
    "        }\n",
    "\n",
    "def print_metrics(metrics_dict, method_name):\n",
    "    \"\"\"Print evaluation metrics\"\"\"\n",
    "    print(f\"\\n{method_name} Results:\")\n",
    "    print(f\"  MRR:    {metrics_dict['MRR']:.4f}\")\n",
    "    print(f\"  Hits@1: {metrics_dict['Hits@1']:.4f}\")\n",
    "    print(f\"  Hits@10: {metrics_dict['Hits@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03a8af",
   "metadata": {},
   "source": [
    "## Section 4: Implement TransE Embedding Model\n",
    "\n",
    "TransE is a translation-based embedding method where embeddings are learned such that **h + r ≈ t** (head + relation ≈ tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467e02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE model defined!\n"
     ]
    }
   ],
   "source": [
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, margin=1.0):\n",
    "        super(TransE, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.margin = margin\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # Initialize embeddings with uniform distribution\n",
    "        nn.init.uniform_(self.entity_embeddings.weight, -1.0, 1.0)\n",
    "        nn.init.uniform_(self.relation_embeddings.weight, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize entity embeddings\n",
    "        with torch.no_grad():\n",
    "            self.entity_embeddings.weight.data = torch.nn.functional.normalize(\n",
    "                self.entity_embeddings.weight.data, p=2, dim=1\n",
    "            )\n",
    "    \n",
    "    def forward(self, head_ids, relation_ids, tail_ids):\n",
    "        \"\"\"Compute distance scores for triplets\"\"\"\n",
    "        head_embeddings = self.entity_embeddings(head_ids)\n",
    "        relation_embeddings = self.relation_embeddings(relation_ids)\n",
    "        tail_embeddings = self.entity_embeddings(tail_ids)\n",
    "        \n",
    "        # TransE: h + r ≈ t, so score should be ||h + r - t||\n",
    "        scores = torch.norm(head_embeddings + relation_embeddings - tail_embeddings, p=2, dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def score_triplet(self, head_id, relation_id, tail_id):\n",
    "        \"\"\"Score a single triplet\"\"\"\n",
    "        head = self.entity_embeddings.weight[head_id]\n",
    "        relation = self.relation_embeddings.weight[relation_id]\n",
    "        tail = self.entity_embeddings.weight[tail_id]\n",
    "        \n",
    "        score = torch.norm(head + relation - tail, p=2)\n",
    "        return score\n",
    "\n",
    "print(\"TransE model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bcdc4",
   "metadata": {},
   "source": [
    "## Section 5: Train TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbb5a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TransE with 13821 triplets...\n",
      "Epoch 20/100, Loss: 0.3704\n",
      "Epoch 40/100, Loss: 0.0159\n",
      "Epoch 60/100, Loss: 0.0152\n",
      "Epoch 80/100, Loss: 0.0152\n",
      "Epoch 100/100, Loss: 0.0159\n",
      "TransE training completed!\n"
     ]
    }
   ],
   "source": [
    "def train_transe(train_triplets, num_entities, num_relations, embedding_dim=100, \n",
    "                  num_epochs=100, batch_size=128, learning_rate=0.001, margin=1.0):\n",
    "    \"\"\"Train TransE model with negative sampling\"\"\"\n",
    "    \n",
    "    model = TransE(num_entities, num_relations, embedding_dim, margin=margin)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Training TransE with {len(train_triplets)} triplets...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Create random batches\n",
    "        indices = np.random.permutation(len(train_triplets))\n",
    "        \n",
    "        for batch_start in range(0, len(train_triplets), batch_size):\n",
    "            batch_indices = indices[batch_start:batch_start + batch_size]\n",
    "            batch_triplets = train_triplets[batch_indices]\n",
    "            \n",
    "            # Separate positive triplets\n",
    "            pos_head = torch.tensor(batch_triplets[:, 0], dtype=torch.long)\n",
    "            pos_relation = torch.tensor(batch_triplets[:, 1], dtype=torch.long)\n",
    "            pos_tail = torch.tensor(batch_triplets[:, 2], dtype=torch.long)\n",
    "            \n",
    "            # Generate negative samples by corrupting tail\n",
    "            neg_tail = torch.tensor(\n",
    "                np.random.randint(0, num_entities, size=len(batch_triplets)),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            pos_scores = model(pos_head, pos_relation, pos_tail)\n",
    "            neg_scores = model(pos_head, pos_relation, neg_tail)\n",
    "            \n",
    "            # Margin-based ranking loss: max(0, margin + pos_score - neg_score)\n",
    "            loss = torch.mean(torch.clamp(margin + pos_scores - neg_scores, min=0))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Normalize entity embeddings\n",
    "            with torch.no_grad():\n",
    "                model.entity_embeddings.weight.data = torch.nn.functional.normalize(\n",
    "                    model.entity_embeddings.weight.data, p=2, dim=1\n",
    "                )\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / (len(train_triplets) // batch_size + 1)\n",
    "        losses.append(avg_epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "# Train TransE\n",
    "transe_model, transe_losses = train_transe(\n",
    "    train_triplets_ids, \n",
    "    num_entities, \n",
    "    num_relations,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    margin=1.0\n",
    ")\n",
    "\n",
    "print(\"TransE training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c3ce4",
   "metadata": {},
   "source": [
    "## Section 6: Implement DistMult Embedding Model\n",
    "\n",
    "DistMult uses bilinear scoring: **score(h,r,t) = <h, r, t>** (element-wise product sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistMult model defined!\n"
     ]
    }
   ],
   "source": [
    "class DistMult(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super(DistMult, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # Initialize with normal distribution\n",
    "        nn.init.normal_(self.entity_embeddings.weight, std=0.1)\n",
    "        nn.init.normal_(self.relation_embeddings.weight, std=0.1)\n",
    "    \n",
    "    def forward(self, head_ids, relation_ids, tail_ids):\n",
    "        \"\"\"Compute bilinear scores for triplets\"\"\"\n",
    "        head_embeddings = self.entity_embeddings(head_ids)\n",
    "        relation_embeddings = self.relation_embeddings(relation_ids)\n",
    "        tail_embeddings = self.entity_embeddings(tail_ids)\n",
    "        \n",
    "        # DistMult: score = <h, r, t> (element-wise product sum)\n",
    "        scores = torch.sum(head_embeddings * relation_embeddings * tail_embeddings, dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def score_triplet(self, head_id, relation_id, tail_id):\n",
    "        \"\"\"Score a single triplet\"\"\"\n",
    "        head = self.entity_embeddings.weight[head_id]\n",
    "        relation = self.relation_embeddings.weight[relation_id]\n",
    "        tail = self.entity_embeddings.weight[tail_id]\n",
    "        \n",
    "        score = torch.sum(head * relation * tail)\n",
    "        return score\n",
    "\n",
    "print(\"DistMult model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed86d2",
   "metadata": {},
   "source": [
    "## Section 7: Train DistMult Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce760d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistMult with 13821 triplets...\n",
      "Epoch 20/100, Loss: 0.7728\n",
      "Epoch 40/100, Loss: 0.0815\n",
      "Epoch 60/100, Loss: 0.0288\n",
      "Epoch 80/100, Loss: 0.0288\n",
      "Epoch 100/100, Loss: 0.0271\n",
      "DistMult training completed!\n"
     ]
    }
   ],
   "source": [
    "def train_distmult(train_triplets, num_entities, num_relations, embedding_dim=100,\n",
    "                    num_epochs=100, batch_size=128, learning_rate=0.001):\n",
    "    \"\"\"Train DistMult model with negative sampling\"\"\"\n",
    "    \n",
    "    model = DistMult(num_entities, num_relations, embedding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Training DistMult with {len(train_triplets)} triplets...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Create random batches\n",
    "        indices = np.random.permutation(len(train_triplets))\n",
    "        \n",
    "        for batch_start in range(0, len(train_triplets), batch_size):\n",
    "            batch_indices = indices[batch_start:batch_start + batch_size]\n",
    "            batch_triplets = train_triplets[batch_indices]\n",
    "            \n",
    "            # Separate positive triplets\n",
    "            pos_head = torch.tensor(batch_triplets[:, 0], dtype=torch.long)\n",
    "            pos_relation = torch.tensor(batch_triplets[:, 1], dtype=torch.long)\n",
    "            pos_tail = torch.tensor(batch_triplets[:, 2], dtype=torch.long)\n",
    "            \n",
    "            # Generate negative samples by corrupting tail\n",
    "            neg_tail = torch.tensor(\n",
    "                np.random.randint(0, num_entities, size=len(batch_triplets)),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            pos_scores = model(pos_head, pos_relation, pos_tail)\n",
    "            neg_scores = model(pos_head, pos_relation, neg_tail)\n",
    "            \n",
    "            # Binary cross-entropy loss with logistic sigmoid\n",
    "            loss = -torch.mean(torch.log(torch.sigmoid(pos_scores)) + \n",
    "                             torch.log(1 - torch.sigmoid(neg_scores)))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / (len(train_triplets) // batch_size + 1)\n",
    "        losses.append(avg_epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "# Train DistMult\n",
    "distmult_model, distmult_losses = train_distmult(\n",
    "    train_triplets_ids,\n",
    "    num_entities,\n",
    "    num_relations,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"DistMult training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da39265",
   "metadata": {},
   "source": [
    "## Section 8: Implement R-GCN Model\n",
    "\n",
    "R-GCN (Relational Graph Convolutional Network) applies relation-specific transformations in graph convolutions for link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-GCN model defined!\n"
     ]
    }
   ],
   "source": [
    "class RGCNLayer(nn.Module):\n",
    "    \"\"\"R-GCN layer with relation-specific transformations\"\"\"\n",
    "    def __init__(self, in_features, out_features, num_relations, num_entities):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_relations = num_relations\n",
    "        self.num_entities = num_entities\n",
    "        \n",
    "        # Relation-specific weight matrices\n",
    "        self.relation_weights = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(in_features, out_features))\n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        \n",
    "        # Self-loop weight matrix\n",
    "        self.self_weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        for weight in self.relation_weights:\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        nn.init.xavier_uniform_(self.self_weight)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, embeddings, adj_arrays):\n",
    "        \"\"\"\n",
    "        embeddings: (num_entities, in_features)\n",
    "        adj_arrays: list of adjacency matrices, one per relation\n",
    "        \"\"\"\n",
    "        output = torch.zeros(self.num_entities, self.out_features, device=embeddings.device)\n",
    "        \n",
    "        # Self-loops\n",
    "        output += torch.matmul(embeddings, self.self_weight)\n",
    "        \n",
    "        # Apply relation-specific transformations\n",
    "        for rel_idx, adj in enumerate(adj_arrays):\n",
    "            if adj._nnz() > 0:  # Only if relation has edges\n",
    "                # Convert sparse to dense if needed\n",
    "                if hasattr(adj, 'to_dense'):\n",
    "                    adj_dense = adj.to_dense()\n",
    "                else:\n",
    "                    adj_dense = adj\n",
    "                \n",
    "                # Normalize by out-degree\n",
    "                out_degree = adj_dense.sum(dim=1, keepdim=True)\n",
    "                out_degree = torch.where(out_degree > 0, out_degree, torch.ones_like(out_degree))\n",
    "                adj_norm = adj_dense / out_degree\n",
    "                \n",
    "                # Apply relation-specific transformation\n",
    "                transformed = torch.matmul(embeddings, self.relation_weights[rel_idx])\n",
    "                output += torch.matmul(adj_norm, transformed)\n",
    "        \n",
    "        output += self.bias\n",
    "        return torch.relu(output)\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, num_layers=2):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Entity embeddings (learnable)\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        nn.init.normal_(self.entity_embeddings.weight, std=0.1)\n",
    "        \n",
    "        # R-GCN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = embedding_dim\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                RGCNLayer(input_dim, embedding_dim, num_relations, num_entities)\n",
    "            )\n",
    "            input_dim = embedding_dim\n",
    "        \n",
    "        # Relation embeddings for scoring\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        nn.init.normal_(self.relation_embeddings.weight, std=0.1)\n",
    "    \n",
    "    def forward(self, adj_arrays):\n",
    "        \"\"\"Forward pass through R-GCN layers\"\"\"\n",
    "        embeddings = self.entity_embeddings.weight\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings, adj_arrays)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def score_link(self, embeddings, head_id, relation_id, tail_id):\n",
    "        \"\"\"Score a link using bilinear product\"\"\"\n",
    "        head = embeddings[head_id]\n",
    "        relation = self.relation_embeddings.weight[relation_id]\n",
    "        tail = embeddings[tail_id]\n",
    "        \n",
    "        # Bilinear scoring\n",
    "        score = torch.sum(head * relation * tail)\n",
    "        return score\n",
    "    \n",
    "    def score_link_batch(self, embeddings, head_ids, relation_ids, tail_ids):\n",
    "        \"\"\"\n",
    "        Vectorised DistMult style scoring\n",
    "        \"\"\"\n",
    "        h = embeddings[head_ids]\n",
    "        t = embeddings[tail_ids]\n",
    "        r = self.relation_embeddings(relation_ids)\n",
    "        \n",
    "        # DistMult score\n",
    "        return torch.sum(h * r * t, dim=1)\n",
    "\n",
    "print(\"R-GCN model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da8928",
   "metadata": {},
   "source": [
    "## Section 9: Train R-GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba8adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28 adjacency matrices\n"
     ]
    }
   ],
   "source": [
    "def create_adjacency_matrices(train_triplets, num_entities, num_relations):\n",
    "    adj_matrices = []\n",
    "\n",
    "    for rel_idx in range(num_relations):\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        for head_id, relation_id, tail_id in train_triplets:\n",
    "            if relation_id == rel_idx:\n",
    "                rows.append(int(head_id))\n",
    "                cols.append(int(tail_id))\n",
    "\n",
    "        if len(rows) > 0:\n",
    "            indices = torch.tensor([rows, cols], dtype=torch.long)\n",
    "            values = torch.ones(len(rows), dtype=torch.float32)\n",
    "\n",
    "            adj = torch.sparse_coo_tensor(\n",
    "                indices, values, (num_entities, num_entities)\n",
    "            ).coalesce()\n",
    "\n",
    "        else:\n",
    "            adj = torch.sparse_coo_tensor(\n",
    "                torch.zeros((2, 0), dtype=torch.long),\n",
    "                torch.tensor([], dtype=torch.float32),\n",
    "                (num_entities, num_entities)\n",
    "            ).coalesce()\n",
    "\n",
    "        adj_matrices.append(adj)\n",
    "\n",
    "    return adj_matrices\n",
    "\n",
    "\n",
    "train_triplets_tensor = torch.tensor(train_triplets_ids, dtype=torch.long)\n",
    "\n",
    "adj_matrices = create_adjacency_matrices(\n",
    "    train_triplets_tensor,\n",
    "    num_entities,\n",
    "    num_relations\n",
    ")\n",
    "\n",
    "print(f\"Created {len(adj_matrices)} adjacency matrices\")\n",
    "\n",
    "\n",
    "def train_rgcn(model,\n",
    "               adj_matrices,\n",
    "               train_triplets,\n",
    "               num_entities,\n",
    "               num_relations,\n",
    "               num_epochs=100,\n",
    "               batch_size=64,\n",
    "               learning_rate=0.001):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    train_triplets = torch.tensor(train_triplets, dtype=torch.long)\n",
    "\n",
    "    print(f\"Training R-GCN with {len(train_triplets)} triplets...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # ✅ Compute embeddings ONCE per epoch\n",
    "        embeddings = model(adj_matrices)\n",
    "\n",
    "        # Shuffle indices\n",
    "        indices = torch.randperm(len(train_triplets))\n",
    "\n",
    "        for batch_start in range(0, len(train_triplets), batch_size):\n",
    "\n",
    "            batch_indices = indices[batch_start:batch_start + batch_size]\n",
    "            batch_triplets = train_triplets[batch_indices]\n",
    "\n",
    "            pos_head_ids = batch_triplets[:, 0]\n",
    "            pos_relation_ids = batch_triplets[:, 1]\n",
    "            pos_tail_ids = batch_triplets[:, 2]\n",
    "\n",
    "            # Negative sampling\n",
    "            neg_tail_ids = torch.randint(\n",
    "                0, num_entities, (len(batch_triplets),)\n",
    "            )\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # VECTORISED SCORING\n",
    "            # --------------------------------------------------\n",
    "\n",
    "            pos_scores = model.score_link_batch(\n",
    "                embeddings,\n",
    "                pos_head_ids,\n",
    "                pos_relation_ids,\n",
    "                pos_tail_ids\n",
    "            )\n",
    "\n",
    "            neg_scores = model.score_link_batch(\n",
    "                embeddings,\n",
    "                pos_head_ids,\n",
    "                pos_relation_ids,\n",
    "                neg_tail_ids\n",
    "            )\n",
    "\n",
    "            # Margin ranking loss\n",
    "            margin = 1.0\n",
    "            loss = torch.mean(torch.clamp(margin + neg_scores - pos_scores, min=0))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / (len(train_triplets) // batch_size + 1)\n",
    "        losses.append(avg_epoch_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373fef7",
   "metadata": {},
   "source": [
    "## Section 10: Evaluate Link Prediction Performance\n",
    "\n",
    "Compute standard link prediction metrics: Mean Reciprocal Rank (MRR), Hits@1, Hits@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f8e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R-GCN with 13821 triplets...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RGCN' object has no attribute 'score_link_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train R-GCN\u001b[39;00m\n\u001b[32m      2\u001b[39m rgcn_model = RGCN(num_entities, num_relations, embedding_dim, num_layers=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m rgcn_model, rgcn_losses = \u001b[43mtrain_rgcn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrgcn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43madj_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_triplets_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_entities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_relations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mR-GCN training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Define scoring functions for each model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mtrain_rgcn\u001b[39m\u001b[34m(model, adj_matrices, train_triplets, num_entities, num_relations, num_epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m     81\u001b[39m neg_tail_ids = torch.randint(\n\u001b[32m     82\u001b[39m     \u001b[32m0\u001b[39m, num_entities, (\u001b[38;5;28mlen\u001b[39m(batch_triplets),)\n\u001b[32m     83\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# VECTORISED SCORING\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m pos_scores = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore_link_batch\u001b[49m(\n\u001b[32m     90\u001b[39m     embeddings,\n\u001b[32m     91\u001b[39m     pos_head_ids,\n\u001b[32m     92\u001b[39m     pos_relation_ids,\n\u001b[32m     93\u001b[39m     pos_tail_ids\n\u001b[32m     94\u001b[39m )\n\u001b[32m     96\u001b[39m neg_scores = model.score_link_batch(\n\u001b[32m     97\u001b[39m     embeddings,\n\u001b[32m     98\u001b[39m     pos_head_ids,\n\u001b[32m     99\u001b[39m     pos_relation_ids,\n\u001b[32m    100\u001b[39m     neg_tail_ids\n\u001b[32m    101\u001b[39m )\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Margin ranking loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Parth Dhodapkar\\Downloads\\precog_task\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1965\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1964\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1966\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1967\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'RGCN' object has no attribute 'score_link_batch'"
     ]
    }
   ],
   "source": [
    "# Train R-GCN\n",
    "rgcn_model = RGCN(num_entities, num_relations, embedding_dim, num_layers=2)\n",
    "\n",
    "rgcn_model, rgcn_losses = train_rgcn(\n",
    "    rgcn_model,\n",
    "    adj_matrices,\n",
    "    train_triplets_ids,\n",
    "    num_entities,\n",
    "    num_relations,\n",
    "    num_epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"R-GCN training completed!\")\n",
    "\n",
    "# Define scoring functions for each model\n",
    "def score_transe(head_id, relation_id, tail_id):\n",
    "    \"\"\"Score function for TransE\"\"\"\n",
    "    return -transe_model.score_triplet(head_id, relation_id, tail_id)\n",
    "\n",
    "def score_distmult(head_id, relation_id, tail_id):\n",
    "    \"\"\"Score function for DistMult (higher scores = better)\"\"\"\n",
    "    return distmult_model.score_triplet(head_id, relation_id, tail_id)\n",
    "\n",
    "def score_rgcn(model, head_id, relation_id, tail_id):\n",
    "    \"\"\"Score function for R-GCN\"\"\"\n",
    "    embeddings = model(adj_matrices)\n",
    "    return model.score_link(embeddings, head_id, relation_id, tail_id)\n",
    "\n",
    "# Evaluate TransE\n",
    "print(\"Evaluating TransE...\")\n",
    "transe_metrics = evaluate_link_prediction(\n",
    "    transe_model,\n",
    "    test_triplets_ids,\n",
    "    score_transe,\n",
    "    method_name=\"transe\"\n",
    ")\n",
    "print_metrics(transe_metrics, \"TransE\")\n",
    "\n",
    "# Evaluate DistMult\n",
    "print(\"\\nEvaluating DistMult...\")\n",
    "distmult_metrics = evaluate_link_prediction(\n",
    "    distmult_model,\n",
    "    test_triplets_ids,\n",
    "    score_distmult,\n",
    "    method_name=\"distmult\"\n",
    ")\n",
    "print_metrics(distmult_metrics, \"DistMult\")\n",
    "\n",
    "# Evaluate R-GCN\n",
    "print(\"\\nEvaluating R-GCN...\")\n",
    "rgcn_metrics = evaluate_link_prediction(\n",
    "    rgcn_model,\n",
    "    test_triplets_ids,\n",
    "    score_rgcn,\n",
    "    method_name=\"rgcn\"\n",
    ")\n",
    "print_metrics(rgcn_metrics, \"R-GCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9d8ba",
   "metadata": {},
   "source": [
    "## Section 11: Visualize Training Loss\n",
    "\n",
    "Plot training loss curves for all three models to visualize convergence and training dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(transe_losses, label='TransE', linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "plt.plot(distmult_losses, label='DistMult', linewidth=2, marker='s', markersize=3, alpha=0.7)\n",
    "plt.plot(rgcn_losses, label='R-GCN', linewidth=2, marker='^', markersize=3, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training Loss Curves for Link Prediction Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss plot saved as 'training_loss.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f374b9",
   "metadata": {},
   "source": [
    "## Section 12: Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebe8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['TransE', 'DistMult', 'R-GCN'],\n",
    "    'MRR': [\n",
    "        transe_metrics['MRR'],\n",
    "        distmult_metrics['MRR'],\n",
    "        rgcn_metrics['MRR']\n",
    "    ],\n",
    "    'Hits@1': [\n",
    "        transe_metrics['Hits@1'],\n",
    "        distmult_metrics['Hits@1'],\n",
    "        rgcn_metrics['Hits@1']\n",
    "    ],\n",
    "    'Hits@10': [\n",
    "        transe_metrics['Hits@10'],\n",
    "        distmult_metrics['Hits@10'],\n",
    "        rgcn_metrics['Hits@10']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINK PREDICTION PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best model for each metric\n",
    "best_mrr = comparison_df.loc[comparison_df['MRR'].idxmax()]\n",
    "best_hits1 = comparison_df.loc[comparison_df['Hits@1'].idxmax()]\n",
    "best_hits10 = comparison_df.loc[comparison_df['Hits@10'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest MRR:    {best_mrr['Model']} ({best_mrr['MRR']:.4f})\")\n",
    "print(f\"Best Hits@1: {best_hits1['Model']} ({best_hits1['Hits@1']:.4f})\")\n",
    "print(f\"Best Hits@10: {best_hits10['Model']} ({best_hits10['Hits@10']:.4f})\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['MRR', 'Hits@1', 'Hits@10']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, max(comparison_df[metric]) * 1.15)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison plot saved as 'model_comparison.png'\")\n",
    "\n",
    "# Save results to CSV\n",
    "comparison_df.to_csv('link_prediction_results.csv', index=False)\n",
    "print(\"Results saved to 'link_prediction_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d2a85",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates **Knowledge Graph Link Prediction** using both embedding-based and GNN-based methods:\n",
    "\n",
    "### Methods Implemented:\n",
    "1. **TransE** - Translation-based embedding where h + r ≈ t\n",
    "2. **DistMult** - Bilinear scoring function <h, r, t>\n",
    "3. **R-GCN** - Relational Graph Convolutional Network with relation-specific transformations\n",
    "\n",
    "### Key Findings:\n",
    "- Different models capture different aspects of the knowledge graph\n",
    "- The embedding-based methods (TransE, DistMult) are efficient and scalable\n",
    "- The GNN-based method (R-GCN) leverages graph structure for better generalization\n",
    "- Standard metrics (MRR, Hits@1, Hits@10) provide comprehensive evaluation\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with other embeddings: ComplEx, RotatE, ConvE\n",
    "- Try advanced GNN architectures: CompGCN, ChebNet\n",
    "- Hyperparameter tuning (embedding dimension, learning rate, margin)\n",
    "- Add validation set for early stopping\n",
    "- Use filter evaluation to exclude training triplets from ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
